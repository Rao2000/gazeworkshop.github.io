---
layout: layout_2022
urltitle:  "GAZE 2022: Gaze Estimation and Prediction in the Wild"
title: "GAZE 2022: Gaze Estimation and Prediction in the Wild"
categories: cvpr, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /2022/
favicon: /2022/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
# Nika_Akin
---

<br>
<div class="row">
  <div class="col-xs-12">
    <img class="img-fluid" src="{{ "img/banner.png" | prepend:site.baseurl }}">
    <small style="float:right;margin-top:1mm;margin-right:12mm;">Image credit to <a href="https://pixabay.com/zh/users/nika_akin-13521770/" target="_blank">Nika_Akin</a></small>
    <br><br>
    <p><center>
      Monday Morning, 20th June 2022 (half-day)
      <!-- Coming soon -->
      <br>
      <!-- 3pm - 8:20pm UTC -->
      <br><br>
      <!-- <table id="top-table">
        <style>
	  #top-table td {
	    padding: 1px 5px;
	  }
	  #top-table td:nth-child(1),
	  #top-table td:nth-child(3) {
	    text-align: right;
	    padding-right: 0px;
	  }
	</style>
        <tr><td>8am   </td><td>- 1:20pm     </td><td> PDT</td><td>(UTC-7)  </td></tr>
        <tr><td>11am  </td><td>- 4:20pm     </td><td> EDT</td><td>(UTC-4)  </td></tr>
        <tr><td>4pm   </td><td>- 9:20pm     </td><td> BST</td><td>(UTC+1)  </td></tr>
        <tr><td>5pm   </td><td>- 10:20pm    </td><td>CEST</td><td>(UTC+2)  </td></tr>
        <tr><td>8:30pm</td><td>- 1:50am (+1)</td><td> IST</td><td>(UTC+5.5)</td></tr>
        <tr><td>11pm  </td><td>- 4:20am (+1)</td><td> CST</td><td>(UTC+8)  </td></tr>
        <tr><td>12am  </td><td>- 5:20am (+1)</td><td> KST</td><td>(UTC+9)  </td></tr>
      </table> -->
      <!-- <br>
      Youtube recording: <a href="https://youtu.be/WQ8azMW_dn8" target="_blank">https://youtu.be/WQ8azMW_dn8</a> -->
    </center></p>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 4th International Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2022) at <a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a> aims to encourage and highlight novel strategies for eye gaze estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, both spatially and temporally.
This is expected to be achieved by applying novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training.
Specifically, the workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Reformulating eye detection, gaze estimation, and gaze prediction pipelines with deep networks.</li>
      <li>Applying geometric and anatomical constraints into the training of (sparse or dense) deep networks.</li>
      <li>Leveraging additional cues such as contexts from face region and head pose information.</li>
      <li>Developing adversarial methods to deal with conditions where current methods fail (illumination, appearance, etc.).</li>
      <li>Exploring attention mechanisms to predict the point of regard.</li>
      <li>Designing new accurate measures to account for rapid eye gaze movement.</li>
      <li>Novel methods for temporal gaze estimation and prediction including Bayesian methods.</li>
      <li>Integrating differentiable components into 3D gaze estimation frameworks.</li>
      <li>Robust estimation from different data modalities such as RGB, depth, head pose, and eye region landmarks.</li>
      <li>Generic gaze estimation method for handling extreme head poses and gaze directions.</li>
      <li>Temporal information usage for eye tracking to provide consistent gaze estimation on the screen.</li>
      <li>Personalization of gaze estimators with few-shot learning.</li>
      <li>Semi-/weak-/un-/self- supervised leraning methods, domain adaptation methods, and other novel methods towards improved representation learning from eye/face region images or gaze target region images.</li>
    </ul>
We will be hosting 2 invited speakers for the topic of gaze estimation. We will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. More information will be provided as soon as possible.
  </div>
</div> <br>

<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://cvpr2022.thecvf.com/author-guidelines" target="_blank">CVPR format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at this <a href="https://cmt3.research.microsoft.com/GAZE2022/Submission/Manage">CMT link</a>.
	    <!-- <a href="https://cmt3.research.microsoft.com/GAZE2021/Submission/Index" target="_blank" title="CMT Submission System for GAZE 2021">this CMT link</a>. -->
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
	    <!--Due to potential clashes with the main conference reviewing schedule, we will accept simultaneous submissions to the ICCV main conference and GAZE Workshop. Simultaneous submissions are otherwise disallowed.-->
          </p>
        </div>
      </div>
    </div>
    <br>
    <!--
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-ea" style="cursor:pointer;">
        <h3 style="margin:0;">Extended Abstracts</h3>
      </div>
      <div id="call-ea" class="panel-collapse collapse in" data-parent="#call">
        <div class="panel-body">
          <p>
            In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ECCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.
          </p>
	  <p>
	    Further details on how this poster session will occur online, is to be decided. In general, we will be following the main ECCV conference guidelines and organization in organizing the presentation of all posters.
          </p>
          <p>Extended abstracts are limited to six pages and must be created using the <a href="https://eccv2020.eu/author-instructions/" target="_blank">official ECCV format</a>. The submission must be sent to <a href="mailto:openeyes.workshop@gmail.com">openeyes.workshop@gmail.com</a> by the deadline (July 17th).
          </p>
          <p>
            We will evaluate and notify authors of acceptance as soon as possible (evaluation on a rolling basis until the deadline) after receiving their extended abstract submission.
          </p>
        </div>
      </div>
    </div>
    <br>
    -->
    <!-- <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-challenge" style="cursor:pointer;">
        <h3 style="margin:0;">GAZE 2021 Challenges</h3>
      </div>
      <div id="call-challenge" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
    Coming soon
	  <p>
	    The GAZE 2021 Challenges are hosted on Codalab, and can be found at:
	  </p>
	  <ul>
	    <li>ETH-XGaze Challenge: <a href="https://competitions.codalab.org/competitions/28930">https://competitions.codalab.org/competitions/28930</a></li>
	    <li>EVE Challenge: <a href="https://competitions.codalab.org/competitions/28954">https://competitions.codalab.org/competitions/28954</a></li>
	  </ul>
	  <p>
            More information on the respective challenges can be found on their pages.
	  </p>
	  <br>
	  <p>
	    We are thankful to our sponsors for providing the following prizes:
	    <table style="width: 100%;">
	      <colgroup>
                <col span="1" style="width: 30%;">
		<col span="1" style="width: 55%;">
		<col span="1" style="width: 15%;">
	      </colgroup>
	      <tbody>
	        <tr>
	          <td><b>ETH-XGaze Challenge Winner</b></td>
	          <td>USD 500</td>
	          <td><small>courtesy of </small><img width="50" src="img/google.png"/></td>
	        </tr>
	        <tr>
	          <td><b>EVE Challenge Winner</b></td>
	          <td>Tobii Eye Tracker 5</td>
	          <td><small>courtesy of </small><img width="50" src="img/tobii.jpg"/></td>
	        </tr>
	      </tbody>
	    </table>
	  </p>
        </div>
      </div>
    </div> -->
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Released</td>
          <td>February 13, 2021</td>
        </tr> -->
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 15, 2022 (23:59 Pacific time)</td>
	  <td><span class="countdown" reference="15 Mar 2022 23:59:59 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>April 1, 2022</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 8, 2022</td>
        </tr>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Closed</td>
          <td>May 28, 2021 (23:59 UTC)</td>
	  <td><span class="countdown" reference="28 May 2022 23:59:59 UTC"></span></td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule (coming soon)</h2>
     <br>
     <!--<p>
       Attending:
       <ul>
         <li>Registered CVPR attendees can find the relevant Zoom and Gatherly links at <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/login.asp">https://www.eventscribe.net/2021/2021CVPR/login.asp</a></li>
	 <li>Others are welcome to join our livestream at <a href="https://youtu.be/ScoHuri_3hs">https://youtu.be/ScoHuri_3hs</a></li>
       </ul>
     </p>-->
     <!-- <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>3:00pm - 3:05pm</td>
          <td class="to-local-time">20 Jun 2021 15:00:00 UTC</td>
          <td>Opening Remarks and Awards</td>
        </tr>
        <tr>
          <td>3:05pm - 3:40pm</td>
          <td class="to-local-time">20 Jun 2021 15:05:00 UTC</td>
          <td>Challenge Winner Talks</td>
        </tr>
        <tr>
          <td>3:40pm - 4:40pm</td>
          <td class="to-local-time">20 Jun 2021 15:40:00 UTC</td>
          <td>Full Workshop Paper Presentations</td>
        </tr>
        <tr>
          <td>4:40pm - 6:00pm</td>
          <td class="to-local-time">20 Jun 2021 16:40:00 UTC</td>
          <td>Break + Poster Session</td>
        </tr>
        <tr>
          <td>6:00pm - 6:35pm</td>
          <td class="to-local-time">20 Jun 2021 18:00:00 UTC</td>
          <td>Keynote Talk: Jim Rehg</td>
        </tr>
        <tr>
          <td>6:35pm - 7:15pm</td>
          <td class="to-local-time">20 Jun 2021 18:35:00 UTC</td>
          <td>Keynote Talk: Moshe Eizenman</td>
        </tr>
        <tr>
          <td>7:10pm - 7:45pm</td>
          <td class="to-local-time">20 Jun 2021 19:10:00 UTC</td>
          <td>Keynote Talk: Adrià Recasens</td>
        </tr>
        <tr>
          <td>7:45pm - 8:15pm</td>
          <td class="to-local-time">20 Jun 2021 19:45:00 UTC</td>
          <td>Panel Discussion</td>
        </tr>
        <tr>
          <td>8:15pm - 8:20pm</td>
          <td class="to-local-time">20 Jun 2021 20:15:00 UTC</td>
          <td>Closing Remarks</td>
        </tr>
      </tbody>
     </table> -->
     <!-- <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span> -->
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://stanford.edu/~gordonwz/">
          <img class="people-pic" src="/2022/img/people/gw.jpg" />
        </a>
        <div class="people-name">
          <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
          <h6>Stanford University</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Eye Tracking Revisited: Applications in Rendering, Displays, Wearable Computing Systems and Emerging Event-based Eye Tracking</h3><br />
        <b>Abstract</b><p class="speaker-abstract">In this talk, we will discuss several emerging applications of eye tracking in AR/VR displays, including ocular parallax rendering and gaze-contingent stereo rendering, as well as wearable computing systems, for example autofocal eyeglasses for presbyopia correction. Moreover, we will discuss emerging technologies for ultra-low-latency eye tracking beyond 10,000 Hz using event sensors.</p>
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
            Gordon Wetzstein is an Associate Professor of <a href="https://ee.stanford.edu/">Electrical Engineering</a> and, by courtesy, of <a href="https://cs.stanford.edu/">Computer Science</a> at <a href="https://www.stanford.edu/">Stanford University</a>. He is the leader of the <a href="https://www.computationalimaging.org/">Stanford Computational Imaging Lab</a> and a faculty co-director of the <a href="https://scien.stanford.edu/">Stanford Center for Image Systems Engineering</a>. At the intersection of computer graphics and vision, computational optics, and applied vision science, Prof. Wetzstein's research has a wide range of applications in next-generation imaging, display, wearable computing, and microscopy systems. Prior to joining Stanford in 2014, Prof. Wetzstein was a Research Scientist at MIT, he received a Ph.D. in Computer Science from the <a href="http://www.cs.ubc.ca/labs/imager/imager.php">University of British Columbia</a> in 2011 and graduated with Honors from the Bauhaus in Weimar, Germany before that. He is the recipient of an NSF CAREER Award, an Alfred P. Sloan Fellowship, an ACM SIGGRAPH Significant New Researcher Award, a Presidential Early Career Award for Scientists and Engineers (PECASE), an SPIE Early Career Achievement Award, a Terman Fellowship, an Okawa Research Grant, the Electronic Imaging Scientist of the Year 2017 Award, an Alain Fournier Ph.D. Dissertation Award, and a Laval Virtual Award as well as Best Paper and Demo Awards at ICCP 2011, 2014, and 2016 and at ICIP 2016.


            </p>
          </div></div>
        </div>
      </div>
    </div>

    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://shenwei1231.github.io/">
          <img class="people-pic" src="/2022/img/people/ws.jpg" />
        </a>
        <div class="people-name">
          <a href="https://shenwei1231.github.io/">Wei Shen</a>
          <h6>Shanghai Jiao Tong University</h6>
        </div>
      </div>
      <div class="col-sm-9">
	<h3>Talk title (TBD)</h3><br />
	<b>Abstract</b><p class="speaker-abstract">
  TBD
	</p>
	<div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#me-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="me-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
            TBD
            </p>
          </div></div>
        </div>
      </div>
    </div>

    <br>

  <!-- <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://people.csail.mit.edu/recasens/">
          <img class="people-pic" src="/2021/img/people/ar.jpg" />
        </a>
        <div class="people-name">
          <a href="https://people.csail.mit.edu/recasens/">Adrià Recasens</a>
          <h6>DeepMind</h6>
        </div>
      </div>
      <div class="col-sm-9">
	<h3>Where are they looking?</h3><br />
	<b>Abstract</b><p class="speaker-abstract">
	In order to understand actions or anticipate intentions, humans need efficient ways of gathering information about each other. In particular, gaze is a rich source of information about other peoples’ activities and intentions. In this talk, we describe our work on predicting human gaze. We introduce a series of methods to follow gaze for different modalities. First, we present GazeFollow, a dataset and model to predict the location of people's gaze in an image. Furthermore, we introduce Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze direction estimation in unconstrained scenes. Finally, we also propose a saliency-based sampling layer designed to improve performance in arbitrary tasks by efficiently zooming into the relevant parts of the input image.
	</p>
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#ar-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="ar-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
	    Adrià Recasens is a Research Scientist at DeepMind. He previously completed his PhD on computer vision at the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in 2019. During his PhD, he worked on various topics related to image and video understanding. Particularly, he has various publications on gaze estimation on image and video. His current research focuses on self-supervised learning specifically applied to multiple modalities such as video, audio or text.
            </p>
          </div></div>
        </div>
      </div>
    </div>
  </div>
</div> -->
  <br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="awards"></a>
    <h2>Awards (will release after reviewing)</h2>
    
  <!-- <div class="award">
      <h3>
	    Best Paper Award
	    <span class="award-sponsor">sponsored by
        <a href="https://www.nvidia.com/" target="_blank"><img src="img/nvidia.jpg" /></a>
	    </span></h3>
      <p><br>
	<b>PupilTAN: A Few-Shot Adversarial Pupil Localizer</b><br>
        <i>Nikolaos Poulopoulos, Emmanouil Z. Psarakis, and Dimitrios Kosmopoulos</i>
      </p>
    </div>

  <div class="award">
      <h3>
	    ETH-XGaze Challenge Winner
	    <span class="award-sponsor">sponsored by
        <a href="https://www.google.com/" target="_blank"><img src="img/google.png" /></a>
	    </span></h3>
      <p><br>
	Team <b>VIPL-TAL-Gaze</b><br>
	<i>Xin Cai, Jiabei Zeng, Yunjia Sun, Xiao Wang, Jiajun Zhang, Boyu Chen, Zhilong Ji, Xiao Liu, Xilin Chen, and Shiguang Shan</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #5</button>
          <a class="btn btn-default" target="_blank" href="https://github.com/VIPL-TAL-GAZE/GAZE2021"><i class="fas fa-code"></i> Code</a>
        </div>
      </p>
    </div>

  <div class="award">
      <h3>
	    EVE Challenge Winner
	    <span class="award-sponsor">sponsored by
        <a href="https://www.tobii.com/" target="_blank"><img src="img/tobii.jpg" /></a>
	    </span></h3>
      <p><br>
	Team <b>HDU_CS</b><br>
	<i>Jun Bao, Buyu Liu, and Jun Yu</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #6</button>
          <a class="btn btn-default" target="_blank" href="https://github.com/bjj9/EVE_SCPT"><i class="fas fa-code"></i> Code</a>
        </div>
      </p>
    </div>
  </div>
</div><br> -->

   <br><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers (will release after reviewing)</h2>
    
  <!-- <div class="paper">
        <span class="title">GOO: A Dataset for Gaze Object Prediction in Retail Environments</span>
        <span class="authors">Henri Tomas, Marcus Reyes, Raimarc Dionido, Mark Ty, Jonric Mirando, Joel Casimiro, Rowel Atienza, and Richard Guinto</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #1</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Tomas_GOO_A_Dataset_for_Gaze_Object_Prediction_in_Retail_Environments_CVPRW_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2105.10793"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">PupilTAN: A Few-Shot Adversarial Pupil Localizer</span>
        <span class="authors">Nikolaos Poulopoulos, Emmanouil Z. Psarakis, and Dimitrios Kosmopoulos</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #2</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Poulopoulos_PupilTAN_A_Few-Shot_Adversarial_Pupil_Localizer_CVPRW_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Appearance-based Gaze Estimation using Attention and Difference Mechanism</span>
        <span class="authors">Murthy L R D and Pradipta Biswas</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #3</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Visual Focus of Attention Estimation in 3D Scene with an Arbitrary Number of Targets</span>
        <span class="authors">Rémy Siegfried and Jean-Marc Odobez</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2021</button>
          <button class="btn btn-poster-id">Poster #4</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Siegfried_Visual_Focus_of_Attention_Estimation_in_3D_Scene_With_an_CVPRW_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
	  <a class="btn btn-default" target="_blank" href="http://publications.idiap.ch/downloads/papers/2021/Siegfried_CVPRW_2021.pdf"><i class="fas fa-archive"></i> IDIAP</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div> -->

  <br><br>

  <a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters (Coming soon)</h2>
    
  <!-- <div class="paper">
        <span class="title">Weakly-Supervised Physically Unconstrained Gaze Estimation</span>
        <span class="authors">Rakshit Kothari, Shalini De Mello, Umar Iqbal, Wonmin Byeon, Seonwook Park, and Jan Kautz</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2021 (Oral)</button>
          <button class="btn btn-poster-id">Poster #7</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Kothari_Weakly-Supervised_Physically_Unconstrained_Gaze_Estimation_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kothari_Weakly-Supervised_Physically_Unconstrained_CVPR_2021_supplemental.pdf"><i class="fas fa-file-pdf"></i> Supp. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2105.09803"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Dual Attention Guided Gaze Target Detection in the Wild</span>
        <span class="authors">Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, Li Song, and Guangtao Zhai</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2021 (Oral)</button>
          <button class="btn btn-poster-id">Poster #8</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Dual_Attention_Guided_Gaze_Target_Detection_in_the_Wild_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Connecting What To Say With Where To Look by Modeling Human Attention Traces</span>
        <span class="authors">Zihang Meng, Licheng Yu, Ning Zhang, Tamara L. Berg, Babak Damavandi, Vikas Singh, and Amy Bearman</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2021</button>
          <button class="btn btn-poster-id">Poster #9</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Meng_Connecting_What_To_Say_With_Where_To_Look_by_Modeling_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Meng_Connecting_What_To_CVPR_2021_supplemental.pdf"><i class="fas fa-file-pdf"></i> Supp. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2105.05964"><i class="fas fa-archive"></i> arXiv</a>
	  <a class="btn btn-default" target="_blank" href="https://github.com/facebookresearch/connect-caption-and-trace"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation</span>
        <span class="authors">Xucong Zhang, Seonwook Park, Thabo Beeler, Derek Bradley, Siyu Tang, and Otmar Hilliges</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">ECCV 2020 (Spotlight)</button>
          <button class="btn btn-poster-id">Poster #10</button>
	  <a class="btn btn-default" target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500358.pdf"><i class="fas fa-file-pdf"></i> PDF (ECVA Open Access)</a>
          <a class="btn btn-default" target="_blank" href="https://ait.ethz.ch/projects/2020/ETH-XGaze/"><i class="fas fa-atom"></i> Project Page</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Towards End-to-end Video-based Eye-Tracking</span>
        <span class="authors">Seonwook Park, Emre Aksan, Xucong Zhang, and Otmar Hilliges</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">ECCV 2020</button>
          <button class="btn btn-poster-id">Poster #11</button>
	  <a class="btn btn-default" target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570732.pdf"><i class="fas fa-file-pdf"></i> PDF (ECVA Open Access)</a>
          <a class="btn btn-default" target="_blank" href="https://ait.ethz.ch/projects/2020/EVE/"><i class="fas fa-atom"></i> Project Page</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div> -->


  </div>
</div>
<br><br>

<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee (Coming soon)</h2>
  </div>
</div>

<!-- <div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://thabobeeler.com/">Thabo Beeler</a><h6>Google</h6></div>
    <div class="people-name"><a target="_blank" href="http://mcbuehler.ch/">Marcel Bühler</a><h6>ETH Zurich</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.cc.gatech.edu/~echong8/">Eunji Chong</a><h6>Georgia Tech</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://research.nvidia.com/person/shalini-gupta/">Shalini De Mello</a><h6>NVIDIA Research</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.tobiasfischer.info/">Tobias Fischer</a><h6>Queensland University of Technology</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.hci.uni-tuebingen.de/chair/team/wolfgang-fuhl">Wolfgang Fuhl</a><h6>University of Tübingen</h6></div>
    <div class="people-name"><a target="_blank" href="https://scholar.google.com/citations?user=ltqAlrgAAAAJ&hl=en">Kenneth Alberto Funes Mora</a><h6>Eyeware Tech SA</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a><h6>ETH Zürich</h6></div>
    <div class="people-name"><a target="_blank" href="http://www.inf.u-szeged.hu/~horanyi/">Nora Horanyi</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyf015.github.io/">Yifei Huang</a><h6>University of Tokyo</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a><h6>University of Birmingham</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://www.biostat.wisc.edu/~yli/">Yin Li</a><h6>University of Wisconsin-Madison</h6></div>
    <div class="people-name"><a target="_blank" href="https://aptx4869lm.github.io/">Miao Liu</a><h6>Georgia Tech</h6></div>
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/spark/">Seonwook Park</a><h6>Lunit Inc.</h6></div>
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a><h6>ETH Zürich</h6></div>
  </div>
</div>
<br> -->

<br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "img/people/hj.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.ccmitss.com/zhang">
      <img class="people-pic" src="{{ "img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>
      <h6>Delft University of Technology</h6>
    </div>
  </div>
  <!-- <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>Lunit Inc.</h6>
    </div>
  </div> -->
  <div class="col-xs-2">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>NVIDIA Research</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://thabobeeler.com/">
      <img class="people-pic" src="{{ "img/people/tb.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://thabobeeler.com/">Thabo Beeler</a>
      <h6>Google</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://www.ecse.rpi.edu/~qji/">
      <img class="people-pic" src="{{ "img/people/qj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ecse.rpi.edu/~qji/">Qiang Ji</a>
      <h6>Rensselaer Polytechnic Institute</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
</div>
